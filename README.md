# VAIA RAG Multi-Tool Agent

An intelligent RAG pipeline with an agentic AI system that automatically detects user intent and routes queries to the appropriate tool:

##  Tools Available

1. **Q&A Tool**: Answers specific questions about the document
2. **Summarizer Tool**: Provides document summaries  
3. **Data Extractor Tool**: Extracts structured data as JSON

##  Quick Start

### Option 1: Docker Deployment (Recommended)

1. **Set up environment variables:**
   Create a `.env` file with:
   ```
   TOGETHER_API_KEY=your_together_key
   COHERE_API_KEY=your_cohere_key
   ```

2. **Run with Docker Compose:**
   ```bash
   docker-compose up --build
   ```

3. **Access the applications:**
   - **Streamlit UI**: [http://localhost:8501](http://localhost:8501)
   - **FastAPI Backend**: [http://localhost:8000](http://localhost:8000)
   - **API Documentation**: [http://localhost:8000/docs](http://localhost:8000/docs)

### Option 2: Local Development

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Set up environment variables:**
   Create a `.env` file with:
   ```
   TOGETHER_API_KEY=your_together_key
   COHERE_API_KEY=your_cohere_key
   ```

3. **Run Streamlit demo:**
   ```bash
   python -m streamlit run src/ui/app.py
   ```

4. **Or run FastAPI backend:**
   ```bash
   python -m uvicorn src.api.main:app --reload
   ```

##  Example Queries

The agent automatically detects intent and routes to the right tool:

**Questions (â†’ Q&A Tool):**
- "What is FutureFlow's market share?"
- "Who are the main competitors?"
- "How is the company performing?"

**Summaries (â†’ Summarizer Tool):**
- "Summarize the document"
- "What are the key points?"
- "Give me an overview"

**Data Extraction (â†’ Extractor Tool):**
- "Extract key financial data"
- "Get market statistics" 
- "Show me the metrics"

## ðŸ§ª Testing the API

Test the FastAPI backend using curl:

```bash
# Q&A Example
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"query": "What is FutureFlow market share?"}'

# Summarization Example  
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"query": "Summarize the document"}'

# Data Extraction Example
curl -X POST "http://localhost:8000/query" \
  -H "Content-Type: application/json" \
  -d '{"query": "Extract financial data"}'
```

## ðŸ“ Project Structure

```
vaia-rag/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ document_loader.py    # Document loading & chunking
â”‚   â”‚   â”œâ”€â”€ context_generator.py  # Contextual chunking
â”‚   â”‚   â”œâ”€â”€ vector_store.py       # FAISS embeddings & search
â”‚   â”‚   â”œâ”€â”€ reranker.py          # Cohere reranking
â”‚   â”‚   â””â”€â”€ rag_pipeline.py      # Main RAG orchestrator
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ tools.py             # LangChain tools (@tool decorators)
â”‚   â”‚   â””â”€â”€ agent.py             # Multi-tool agent with LangGraph
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ main.py              # FastAPI backend
â”‚   â””â”€â”€ ui/
â”‚       â””â”€â”€ app.py               # Streamlit frontend
â”œâ”€â”€ data/
â”‚   â””â”€â”€ document.txt             # Sample document
â”œâ”€â”€ Dockerfile                   # Container configuration
â”œâ”€â”€ docker-compose.yml          # Multi-service deployment
â””â”€â”€ requirements.txt             # Python dependencies
```

## ðŸŽ¯ Key Features

- **Intelligent routing**: Agent automatically detects user intent and selects the right tool.
- **Tool-specific prompts**: Optimized prompts per tool. Few-shot prompting for the Data Extractor enforces strict JSON output.
- **Contextual chunking**: Adds additional context to each chunk for better retrieval than naive size-based chunking.
- **Reranking**: Uses Cohere to rank top-k (k=5) results for better answers.
- **RAG pipeline**: Separate modules for each stage to keep the system scalable.

```
ðŸ“„ Document â†’ âœ‚ï¸ Chunks â†’ ðŸ§  Context â†’ ðŸ”¢ Vectors â†’ ðŸ’¾ Store
                                                        â†“
ðŸ‘¤ User Query â†’ ðŸ” Search â†’ ðŸ“Š Rerank â†’ ðŸ¤– LLM â†’ âœ… Answer
```

### 1. Document ingestion

```
"Innovate Inc. has 12% market share"
    â†“ (chunk)
"This chunk discusses market share data for Innovate Inc. Content: Innovate Inc. has 12% market share"
    â†“ (embed)
[0.1, 0.4, 0.7, ...] (vector representation)
```

### 2. Retrieving relevant results

```
User: "What is market share?"
    â†“ (embed query)
[0.2, 0.3, 0.8, ...]
    â†“ (similarity search)
Find chunks with similar vectors
    â†“ (rerank)
Return top 5 most relevant chunks
```

### 3. Why Together AI

```
Due to credit expiry of openai and pricing I used Together.ai as it provides many models for both chat and embeddings. 
This project uses 'meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo' as it is not heavy for local deployment and it is particularly good at summarization and code generation. 
It is also efficient more on consumer grade GPU as its context length is not too large as well -> 128K


For embeddings I used 'BAAI/bge-large-en-v1.5' as its popular for its embeddings for tasks like RAG and the workflow is RAG based.
```